{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6vzyJ9_j2JA"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6xssrhDj_4V"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMLAYYpfkaW8"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d kazanova/sentiment140"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mnh1x-57kMdf"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "dataset = '/content/sentiment140.zip'\n",
        "\n",
        "with ZipFile(dataset,'r') as zip:\n",
        " zip.extractall()\n",
        "print('data extracted')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okl4ZyVrkP7Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import re, string\n",
        "import emoji\n",
        "import nltk\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import TFBertModel\n",
        "from transformers import RobertaTokenizerFast\n",
        "from transformers import TFRobertaModel\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "seed=42\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.despine()\n",
        "\n",
        "plt.rc(\"figure\", autolayout=True)\n",
        "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7huprv0Jl3f3"
      },
      "outputs": [],
      "source": [
        "def conf_matrix(y, y_pred, title):\n",
        "    fig, ax = plt.subplots(figsize=(5,5))\n",
        "    labels = ['Negative', 'Positive']\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    ax = sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":25})\n",
        "    plt.title(title, fontsize=20)\n",
        "    ax.xaxis.set_ticklabels(labels, fontsize=17)\n",
        "    ax.yaxis.set_ticklabels(labels, fontsize=17)\n",
        "    ax.set_ylabel('Actual', fontsize=20)\n",
        "    ax.set_xlabel('Predicted', fontsize=20)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**PRE-PROCESSING IF DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79OgcsbWk0pP"
      },
      "outputs": [],
      "source": [
        "col_name = ['target','id','date','flag','user','text']\n",
        "twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',names= col_name ,encoding = 'ISO-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavzMzUynShy"
      },
      "outputs": [],
      "source": [
        "twitter_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tckhRaXsnYxP"
      },
      "outputs": [],
      "source": [
        "twitter_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3KOkTwEnn9B"
      },
      "outputs": [],
      "source": [
        "twitter_data['date'] = pd.to_datetime(twitter_data['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFGCtQtRn5So"
      },
      "outputs": [],
      "source": [
        "twitter_data.drop_duplicates(subset=['text'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahU968L5oMNV"
      },
      "outputs": [],
      "source": [
        "twitter_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3pzx9L3oP7s"
      },
      "outputs": [],
      "source": [
        "tweets_perday = twitter_data['date'].value_counts().sort_index().reset_index()\n",
        "tweets_perday.columns = ['date', 'counts']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsFPNxuio1xp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import emoji\n",
        "\n",
        "def strip_emoji(text):\n",
        "    return emoji.replace_emoji(text, replace='')\n",
        "\n",
        "def strip_all_entities(text):\n",
        "    text = text.replace('\\r', '').replace('\\n', ' ').lower() \n",
        "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)        \n",
        "    text = re.sub(r'[^\\x00-\\x7f]',r'', text)                \n",
        "    banned_list = string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n",
        "    table = str.maketrans('', '', banned_list)\n",
        "    text = text.translate(table)\n",
        "    return text\n",
        "\n",
        "def clean_hashtags(tweet):\n",
        "    new_tweet = \" \".join(\n",
        "        word.strip() for word in re.split(\n",
        "            '#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet\n",
        "        )\n",
        "    ) \n",
        "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) \n",
        "    return new_tweet2\n",
        "\n",
        "def filter_chars(a):\n",
        "    sent = []\n",
        "    for word in a.split(' '):\n",
        "        if ('$' in word) or ('&' in word):\n",
        "            sent.append('')\n",
        "        else:\n",
        "            sent.append(word)\n",
        "    return ' '.join(sent)\n",
        "\n",
        "def remove_mult_spaces(text):\n",
        "    return re.sub(r\"\\s\\s+\", \" \", text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqw6mW71vgFJ"
      },
      "outputs": [],
      "source": [
        "texts_new = []\n",
        "for t in twitter_data.text:\n",
        "    texts_new.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbfCp5sYvrhI"
      },
      "outputs": [],
      "source": [
        "twitter_data['cleaned_data'] = texts_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orjJcKAEwzrV"
      },
      "outputs": [],
      "source": [
        "twitter_data['cleaned_data'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUxHB1SRxCPj"
      },
      "outputs": [],
      "source": [
        "text_len = []\n",
        "for text in twitter_data.cleaned_data:\n",
        "    tweet_len = len(text.split())\n",
        "    text_len.append(tweet_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bkUKs_jxkne"
      },
      "outputs": [],
      "source": [
        "twitter_data['text_len'] = text_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqf8KWvMxtel"
      },
      "outputs": [],
      "source": [
        "twitter_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKieUJImx3Q3"
      },
      "outputs": [],
      "source": [
        "twitter_data = twitter_data[twitter_data['text_len'] > 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNR-u1V_yLjT"
      },
      "outputs": [],
      "source": [
        "twitter_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Training data deeper cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2noELR9ZyOU8"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6VMtQQ4yaDr"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in twitter_data['cleaned_data'].values:\n",
        "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
        "    token_lens.append(len(tokens))\n",
        "\n",
        "max_len=np.max(token_lens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvGjtkNkyplm"
      },
      "outputs": [],
      "source": [
        "print(f\"MAX TOKENIZED SENTENCE LENGTH: {max_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5-Au4wh29B5"
      },
      "outputs": [],
      "source": [
        "twitter_data['target'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faPVOxZj3xYE"
      },
      "outputs": [],
      "source": [
        "twitter_data.replace({'target':{4:1}},inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPn6PwPA394O"
      },
      "outputs": [],
      "source": [
        "twitter_data['target'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Class Balancing by RandomOverSampler**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_PUrpvq4xZk"
      },
      "outputs": [],
      "source": [
        "import builtins\n",
        "ros = RandomOverSampler()\n",
        "train_x, train_y = ros.fit_resample(\n",
        "    np.array(twitter_data['cleaned_data']).reshape(-1, 1),\n",
        "    np.array(twitter_data['target']).reshape(-1, 1)\n",
        ")\n",
        "\n",
        "train_y = train_y.ravel() \n",
        "\n",
        "train_os = pd.DataFrame(\n",
        "    list(builtins.zip([x[0] for x in train_x], train_y)),\n",
        "    columns=['cleaned_data', 'target']\n",
        ")\n",
        "print(\"\\nAfter oversampling:\")\n",
        "print(train_os)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6HQoKmk5gUO"
      },
      "outputs": [],
      "source": [
        "train_os['target'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**SPLITING THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7g_P6hbl7J8e"
      },
      "outputs": [],
      "source": [
        "X = train_os['cleaned_data'].values\n",
        "y = train_os['target'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4l4T4_d7bKW"
      },
      "outputs": [],
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SH91O1j8JM_"
      },
      "outputs": [],
      "source": [
        "X_valid, X_test, y_valid, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c6XNtaO8WzZ"
      },
      "outputs": [],
      "source": [
        "X_train.shape, X_valid.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**One hot encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EKGXuOP8gF8"
      },
      "outputs": [],
      "source": [
        "y_train_le = y_train.copy()\n",
        "y_valid_le = y_valid.copy()\n",
        "y_test_le = y_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kclT4S8C8v8c"
      },
      "outputs": [],
      "source": [
        "ohe = preprocessing.OneHotEncoder()\n",
        "y_train = ohe.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
        "y_valid = ohe.fit_transform(np.array(y_valid).reshape(-1, 1)).toarray()\n",
        "y_test = ohe.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**RoBERTa Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyTttYAH84_W"
      },
      "outputs": [],
      "source": [
        "tokenizer_roberta = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4q6e5lV9S67"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in X_train:\n",
        "    tokens = tokenizer_roberta.encode(txt, max_length=512, truncation=True)\n",
        "    token_lens.append(len(tokens))\n",
        "max_length=np.max(token_lens)\n",
        "max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8QtigCW9ZwW"
      },
      "outputs": [],
      "source": [
        "MAX_LEN=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Af42lAF895od"
      },
      "outputs": [],
      "source": [
        "def tokenize_roberta(data,max_len=MAX_LEN) :\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for i in range(len(data)):\n",
        "        encoded = tokenizer_roberta.encode_plus(\n",
        "            data[i],\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    return np.array(input_ids),np.array(attention_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aopAoVlI9_gN"
      },
      "outputs": [],
      "source": [
        "train_input_ids, train_attention_masks = tokenize_roberta(X_train, MAX_LEN)\n",
        "val_input_ids, val_attention_masks = tokenize_roberta(X_valid, MAX_LEN)\n",
        "test_input_ids, test_attention_masks = tokenize_roberta(X_test, MAX_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTJ58z_I-Hvv"
      },
      "outputs": [],
      "source": [
        "def create_model(bert_model, max_len=MAX_LEN):\n",
        "\n",
        "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5, decay=1e-7)\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "    accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "    output = bert_model([input_ids,attention_masks])\n",
        "    output = output[1]\n",
        "    output = tf.keras.layers.Dense(2, activation=tf.nn.softmax)(output)\n",
        "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
        "    model.compile(opt, loss=loss, metrics=accuracy)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf8et-B9An2M"
      },
      "outputs": [],
      "source": [
        "roberta_model = TFRobertaModel.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_alcOBdAs86"
      },
      "outputs": [],
      "source": [
        "model = create_model(roberta_model, MAX_LEN)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN4-VisUt1Pm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def subsample_data(input_ids, attention_masks, labels, n_samples):\n",
        "    \"\"\"\n",
        "    Randomly subsample input_ids, attention_masks, and labels to n_samples.\n",
        "    \"\"\"\n",
        "    n_samples = min(n_samples, len(input_ids))\n",
        "    np.random.seed(42)  \n",
        "    indices = np.random.choice(len(input_ids), size=n_samples, replace=False)\n",
        "    return input_ids[indices], attention_masks[indices], labels[indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIsZ7rAVutqQ"
      },
      "outputs": [],
      "source": [
        "n_train = 100_000\n",
        "n_val = 10_000\n",
        "n_test = 10_000\n",
        "\n",
        "train_input_ids_small, train_attention_masks_small, y_train_small = subsample_data(\n",
        "    train_input_ids, train_attention_masks, y_train, n_train\n",
        ")\n",
        "\n",
        "val_input_ids_small, val_attention_masks_small, y_valid_small = subsample_data(\n",
        "    val_input_ids, val_attention_masks, y_valid, n_val\n",
        ")\n",
        "\n",
        "test_input_ids_small, test_attention_masks_small, y_test_small = subsample_data(\n",
        "    test_input_ids, test_attention_masks, y_test, n_test\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HFQJReNu4vW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint('/content/checkpoint_model.h5',\n",
        "                             save_best_only=False,  \n",
        "                             save_freq=100)         \n",
        "\n",
        "history = model.fit(\n",
        "    [train_input_ids_small, train_attention_masks_small],\n",
        "    y_train_small,\n",
        "    validation_data=([val_input_ids_small, val_attention_masks_small], y_valid_small),\n",
        "    epochs=4, batch_size=32,\n",
        "    callbacks=[checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**RESULTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSarqO5dA8dx"
      },
      "outputs": [],
      "source": [
        "result_roberta = model.predict([test_input_ids,test_attention_masks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nelYB8CDBzM8"
      },
      "outputs": [],
      "source": [
        "y_pred_roberta =  np.zeros_like(result_roberta)\n",
        "y_pred_roberta[np.arange(len(y_pred_roberta)), result_roberta.argmax(1)] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFjZy6QQI8mV"
      },
      "outputs": [],
      "source": [
        "conf_matrix(y_test.argmax(1),y_pred_roberta.argmax(1),'RoBERTa Sentiment Analysis\\nConfusion Matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aap3-f0MJCyC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_test_labels = np.argmax(y_test, axis=1) if y_test.ndim > 1 else y_test\n",
        "y_pred_labels = np.argmax(y_pred_roberta, axis=1) if y_pred_roberta.ndim > 1 else y_pred_roberta\n",
        "\n",
        "print('\\tClassification Report for RoBERTa:\\n\\n',\n",
        "      classification_report(y_test_labels, y_pred_labels, target_names=['Negative', 'Positive']))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
